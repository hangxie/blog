{"id":907,"date":"2010-01-19T15:11:25","date_gmt":"2010-01-19T22:11:25","guid":{"rendered":"http:\/\/xiehang.com\/blog\/?p=907"},"modified":"2010-01-19T15:11:25","modified_gmt":"2010-01-19T22:11:25","slug":"cassandras-read-performance","status":"publish","type":"post","link":"https:\/\/xiehang.com\/blog\/2010\/01\/19\/cassandras-read-performance\/","title":{"rendered":"Cassandra&#8217;s read performance"},"content":{"rendered":"<p>I setup a testing environment on couple of company boxes to see how Cassandra performs with real machines (real here means powerful enough to be a data node), here are details of the environment:<\/p>\n<ul>\n<li>Two client nodes, one server nodes, all are RHEL 4.x. I use two clients nodes as I found that during the performance test, single client machine is unable to generate enough load<\/li>\n<li>All three machines are 8 cores\/16G memory (well, memory is not a big deal for my tests)<\/li>\n<li>Running Cassandray 0.5.0 RC3 (built from svn last night)<\/li>\n<li>Client is using Python<\/li>\n<\/ul>\n<p>Here is the graph for simple request (single key lookup):<a href=\"https:\/\/xiehang.com\/blog\/wp-content\/uploads\/2010\/01\/simple.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"size-medium wp-image-908    alignright\" title=\"Simple lookup\" src=\"https:\/\/xiehang.com\/blog\/wp-content\/uploads\/2010\/01\/simple-300x200.jpg\" alt=\"\" width=\"300\" height=\"200\" srcset=\"https:\/\/xiehang.com\/blog\/wp-content\/uploads\/2010\/01\/simple-300x200.jpg 300w, https:\/\/xiehang.com\/blog\/wp-content\/uploads\/2010\/01\/simple.jpg 516w\" sizes=\"(max-width: 300px) 100vw, 300px\" \/><\/a><\/p>\n<p>It seems the result is pretty encouraging &#8211; query per second of the server is growing almost linearly, at about 5,000 QPS, over CPU utilization is still under 40% (25% user, 12% sys), I cannot get more client boxes to test, but if it goes this way, and let&#8217;s make 80% is threshold of CPU utilization, then this kind of box can handle 10K QPS, roughly, with latency at around 3ms.<\/p>\n<p>Note that CPU utilization, QPS per client, and latency is not quite clear as the overall QPS is too high, but you can get some ideas from next graph &#8230;<\/p>\n<p>Here is the graph for application (login, which will do one user lookup, and then 10~100 user lookups, each lookup is to get one buddy&#8217;s information):<a href=\"https:\/\/xiehang.com\/blog\/wp-content\/uploads\/2010\/01\/app.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"alignright size-medium wp-image-909\" title=\"Login Operation\" src=\"https:\/\/xiehang.com\/blog\/wp-content\/uploads\/2010\/01\/app-300x200.jpg\" alt=\"\" width=\"300\" height=\"200\" srcset=\"https:\/\/xiehang.com\/blog\/wp-content\/uploads\/2010\/01\/app-300x200.jpg 300w, https:\/\/xiehang.com\/blog\/wp-content\/uploads\/2010\/01\/app.jpg 516w\" sizes=\"(max-width: 300px) 100vw, 300px\" \/><\/a><\/p>\n<p>The result is kind of worrying me, since the CPU utilization is 70% already (45% user and 25% sys), it seems 200 QPS is what the cluster can provide. However, thinking of the login operation is doing way too many table lookups (average 55 lookups per login), so just matches the simple lookup we discussed above (10K QPS per box), while latency is at around 80ms.<\/p>\n<p>Actually, 20% sys is pretty bad, means the kernel is busy switching (I didn&#8217;t check vmstat during that time, but this is a reasonable guess), but again, this may be reasonable since the machine is handling 16 active clients who are sending bunch of requests, while it has only 8 physical cores so context switching is unavoidable.<\/p>\n<p>Since everything&#8217;s linear, I can assume 4 cores boxes can offer 5,000 QPS with reasonable latency. I will do some similar tests with MySQL and memcached, and I will do similar test with multiple data nodes as well, since I got impression that multiple data nodes is far slower than single node (inter-node communication?).<\/p>\n","protected":false},"excerpt":{"rendered":"<p>I setup a testing environment on couple of company boxes to see how Cassandra performs with real machines (real here means powerful enough to be a data node), here are details of the environment: Two client nodes, one server nodes, all are RHEL 4.x. I use two clients nodes as I found that during the <a href='https:\/\/xiehang.com\/blog\/2010\/01\/19\/cassandras-read-performance\/' class='excerpt-more'>[&#8230;]<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"comment_status":"closed","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"footnotes":""},"categories":[1],"tags":[155,141,28],"_links":{"self":[{"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/posts\/907"}],"collection":[{"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/comments?post=907"}],"version-history":[{"count":2,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/posts\/907\/revisions"}],"predecessor-version":[{"id":911,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/posts\/907\/revisions\/911"}],"wp:attachment":[{"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/media?parent=907"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/categories?post=907"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/tags?post=907"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}
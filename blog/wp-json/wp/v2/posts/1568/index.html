{"id":1568,"date":"2013-08-05T15:58:45","date_gmt":"2013-08-05T22:58:45","guid":{"rendered":"http:\/\/xiehang.com\/blog\/?p=1568"},"modified":"2014-01-28T11:05:17","modified_gmt":"2014-01-28T18:05:17","slug":"playing-with-gluster-file-system","status":"publish","type":"post","link":"https:\/\/xiehang.com\/blog\/2013\/08\/05\/playing-with-gluster-file-system\/","title":{"rendered":"Playing with Gluster File System"},"content":{"rendered":"<p>Working on glusterfs test environment as mentioned <a href=\"https:\/\/xiehang.com\/blog\/2013\/08\/02\/planning-glusterfs\/\">here<\/a>, so far everything&#8217;s working with some headache.<\/p>\n<ol>\n<li>have 4 nodes up and running and join into same pool, make all extra disks (sdb, sdc, sdd) XFS (fdisk then mkfs, so sdb mentioned below is actually sdb1)<\/li>\n<li>create distributed-replicated volume gfs_v0 with gfs11:sdb+gfs12:sdb and gfs11:sdc+gfs12:sdc<\/li>\n<li>mount gfs on all these 4 boxes (I don&#8217;t have dedicated client hosts &#8230;)<\/li>\n<li>copied 16 450M~500M tgz files to gfs_v0, everything looked fine<\/li>\n<p><!--more--><\/p>\n<li>untar two tgz files to gfs_v0, result in 256 directories, with ~1100 files in each directory, it was working smoothly, though the performance is not that great, but I think this is a known issue, or by design to behave like this<\/li>\n<li>add gfs13:sdd+gfs14:sdd to gfs_v0, then launch rebalance, both worked as expected and rebalance is faster than I was expecting (compare with the untar in step #5, but I think glusterfs is actually writing the XFS disks so performance is better)<\/li>\n<li>here comes the trouble case &#8211; since current layout does not meet my requirement as mentioned in <a href=\"https:\/\/xiehang.com\/blog\/2013\/08\/02\/planning-glusterfs\/\">planning page<\/a>, I need to use gfs13:sdb replace gfs12:sdc, &#8220;replace-brick start&#8221; without problem, but &#8220;replace-brick status&#8221; told me &#8220;cannot commit on localhost&#8221; or something similar, it seems this is a known issue and lots of people got hit by this, but so far I haven&#8217;t found any fix yet<\/li>\n<li>kick out a &#8220;replace-brick commit force&#8221; (maybe I did wrong in this step, let&#8217;s see), and vol info shows that the replacement was done, but obviously without data properly transferred<\/li>\n<li>I think I should do a heal at that moment, but who knows, I launched rebalance, which seems ok and healing is ongoing as well, but after ~10 hours it is still on-going, so I stopped rebalance, and launched heal to make sure at least the volume is good enough to handle disaster. It was really fast so I guess most stuffs got fixed in the rebalance stage.<\/li>\n<li>add three more pairs of bricks gfs11:sdd+gfs14:sdb, gfs12:sdc+gfs13:sdc, and gfs12:sdd+gfs14:sdc, and launched rebalance<\/li>\n<\/ol>\n<p>I&#8217;m waiting for the last rebalance to be finished and then I&#8217;m going to create some disasters :D. Things like power off a node, power cycle a node, fail a disk, all these sort of stuffs and let&#8217;s see what happen then.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>Working on glusterfs test environment as mentioned here, so far everything&#8217;s working with some headache. have 4 nodes up and running and join into same pool, make all extra disks (sdb, sdc, sdd) XFS (fdisk then mkfs, so sdb mentioned below is actually sdb1) create distributed-replicated volume gfs_v0 with gfs11:sdb+gfs12:sdb and gfs11:sdc+gfs12:sdc mount gfs on <a href='https:\/\/xiehang.com\/blog\/2013\/08\/05\/playing-with-gluster-file-system\/' class='excerpt-more'>[&#8230;]<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"comment_status":"closed","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"footnotes":""},"categories":[1],"tags":[437],"_links":{"self":[{"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/posts\/1568"}],"collection":[{"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/comments?post=1568"}],"version-history":[{"count":2,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/posts\/1568\/revisions"}],"predecessor-version":[{"id":1646,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/posts\/1568\/revisions\/1646"}],"wp:attachment":[{"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/media?parent=1568"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/categories?post=1568"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/xiehang.com\/blog\/wp-json\/wp\/v2\/tags?post=1568"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}
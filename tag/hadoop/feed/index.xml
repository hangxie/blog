<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>hadoop &#8211; Flying Bug</title>
	<atom:link href="https://xiehang.com/blog/tag/hadoop/feed/" rel="self" type="application/rss+xml" />
	<link>https://xiehang.com/blog</link>
	<description>Debugging and Being Debugged</description>
	<lastBuildDate>Tue, 28 Jan 2014 18:10:50 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0.1</generator>
	<item>
		<title>Moving back to Hadoop world</title>
		<link>https://xiehang.com/blog/2012/12/21/moving-back-to-hadoop-world/</link>
					<comments>https://xiehang.com/blog/2012/12/21/moving-back-to-hadoop-world/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Fri, 21 Dec 2012 17:12:51 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[hadoop]]></category>
		<category><![CDATA[site operation]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1452</guid>

					<description><![CDATA[Guess I will spend some time on Hadoop and Hive/HBase in the coming year, so I&#8217;m preparing &#8230; It seems Hadoop is doing pretty good, documentation is getting better, and people are thinking more about site operation, this means it is no longer a toy. Things like access control, seperation of daemon users, and backward <a href='https://xiehang.com/blog/2012/12/21/moving-back-to-hadoop-world/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Guess I will spend some time on Hadoop and Hive/HBase in the coming year, so I&#8217;m preparing &#8230;</p>
<p>It seems Hadoop is doing pretty good, documentation is getting better, and people are thinking more about site operation, this means it is no longer a toy. Things like access control, seperation of daemon users, and backward compatibility, all these stuffs are making it a serious (enterprise???) tool.</p>
<p>Why I care about site operation so much? I&#8217;m seriously doubting if I&#8217;m on dev side or ops side &#8230;</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2012/12/21/moving-back-to-hadoop-world/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>Hadoop streaming errors</title>
		<link>https://xiehang.com/blog/2012/06/25/hadoop-streaming-errors/</link>
					<comments>https://xiehang.com/blog/2012/06/25/hadoop-streaming-errors/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Tue, 26 Jun 2012 00:12:28 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[access]]></category>
		<category><![CDATA[hadoop]]></category>
		<category><![CDATA[queue]]></category>
		<category><![CDATA[streaming]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1358</guid>

					<description><![CDATA[A newbie note. I got this while running a streaming job: 12/06/26 05:43:05 ERROR streaming.StreamJob: Error Launching job : java.io.IOException: java.lang.NullPointerException at org.apache.hadoop.mapred.QueueManager.getQueueACL(QueueManager.java:382) at org.apache.hadoop.mapred.JobTracker.getQueueAdmins(JobTracker.java:4444) at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1434) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1430) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1428) Turned out it&#8217;s an ACL problem. Running &#8220;hadoop <a href='https://xiehang.com/blog/2012/06/25/hadoop-streaming-errors/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>A newbie note.</p>
<p>I got this while running a streaming job:</p>
<p>12/06/26 05:43:05 ERROR streaming.StreamJob: Error Launching job : java.io.IOException: java.lang.NullPointerException<br />
at org.apache.hadoop.mapred.QueueManager.getQueueACL(QueueManager.java:382)<br />
at org.apache.hadoop.mapred.JobTracker.getQueueAdmins(JobTracker.java:4444)<br />
at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)<br />
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />
at java.lang.reflect.Method.invoke(Method.java:597)<br />
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557)<br />
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1434)<br />
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1430)<br />
at java.security.AccessController.doPrivileged(Native Method)<br />
at javax.security.auth.Subject.doAs(Subject.java:396)<br />
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)<br />
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1428)</p>
<p>Turned out it&#8217;s an ACL problem. Running &#8220;hadoop queue -showacls&#8221; gives me the list of queues that I have access, then relaunch the task with:</p>
<p>hadoop jar $HADOOP_HOME/contrib/streaming/hadoop-streaming-0.20.2-cdh3u1.jar \<br />
-Dmapred.job.queue.name=queue_i_have_access \<br />
&#8230;.</p>
<p>and everything runs smootly.</p>
<p>I will post more newbie comments here.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2012/06/25/hadoop-streaming-errors/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
		<item>
		<title>nutch, solr, term vector, and so on</title>
		<link>https://xiehang.com/blog/2012/02/08/nutch-solr-term-vector-and-so-on/</link>
					<comments>https://xiehang.com/blog/2012/02/08/nutch-solr-term-vector-and-so-on/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Thu, 09 Feb 2012 02:01:20 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[hadoop]]></category>
		<category><![CDATA[mmseg4j]]></category>
		<category><![CDATA[nutch]]></category>
		<category><![CDATA[solr]]></category>
		<category><![CDATA[term extraction]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1256</guid>

					<description><![CDATA[Just finished a prototype of terminology extraction based on nutch and solr, check test page. I also have another (quick and dirty) script to inject new URLs into nutch and then solr, the whole demo is not finished yet since I need to put up something to remove outdated pages (what is outdated?). The work <a href='https://xiehang.com/blog/2012/02/08/nutch-solr-term-vector-and-so-on/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Just finished a prototype of terminology extraction based on <a href="http://nutch.apache.org/">nutch</a> and <a href="http://lucene.apache.org/solr/">solr</a>, check <a href="http://solr.xiehang.com/solr/select/?&#038;qt=tvrh&#038;q=id%3Ahttp%5C%3A%2F%2Fnews.sohu.com%2F&#038;fl=content&#038;tv.tf_idf=true">test page</a>.</p>
<p>I also have another (quick and dirty) script to inject new URLs into nutch and then solr, the whole demo is not finished yet since I need to put up something to remove outdated pages (what is outdated?).</p>
<p>The work flow should be something like this:<span id="more-1256"></span></p>
<ol>
<li>Get URL from client&#8217;s request</li>
<li>If the URL had been crawled and it is not outdated (not sure how to define this yet), return the terms and we are all set (exit 0!!! <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f600.png" alt="ðŸ˜€" class="wp-smiley" style="height: 1em; max-height: 1em;" /> )</li>
<li>kick the URL to the crawl queue so that it can be crawled later on</li>
<li>return an empty term list back to client (sorry, we don&#8217;t know too much about the page, yet</li>
</ol>
<p>Some improvements in my mind include:</p>
<ol>
<li>Whenever we haven&#8217;t crawled the page yet, the front end (JS in the web page) should be able to grab some keywords from the web page itself, the use them as terms
<ul>
<li>This may be resource consuming in browser, but I need to do some more tests</li>
<li>The JS has to be run at the very last of the web page so that it can get most text from the page, this may slower down the business logic, and could not be acceptable</li>
<li>Sure, we cannot put too many fancy logics in the JS so the result could be very bad</li>
<li>Since JS is clear text to everyone so it (I mean the engine) can be fooled by the web page owner, whenever they have motivation (such as getting more money? I&#8217;m not sure)</i>
</ul>
</li>
<li>The default TermVectorComponent does not give me sorted result, so sooner or later we need to have a customized class to handle this, I assume that we will deliver better result</li>
<li>I&#8217;m using <a href="http://code.google.com/p/mmseg4j/">mmseg4j</a> as the tokenizer, this may not be the best solution but I have no choice so far (again, quick and dirty), in the future if we cannot find something better, we should at least revise the dictionary to make things work better</li>
<li>I prefer a C/C++ solution for this but I don&#8217;t think I can get one, however, I will wrap everything into a C/C++ library so that it can fit into other systems easily</li>
<li>My demo deployment is running on a AWS micro node, with nutch in local mode, based on requirements I got, this should be ok since I don&#8217;t care about inter-document (page) relationship, however, if this becomes a requirement, then I have to run in deploy mode which means I need a hadoop cluster</li>
<li>On solr side, I will do some calculation to see if I need a hadoop or not. It was said we won&#8217;t get web pages more than 100 millions so it maybe good enough to leave it as-is, but then I have to think about redundancy and failover, disaster recovery, and so on.</li>
</ol>
<p>I guess I will still need a hadoop cluster, even it is small, at least I can handle part of my SLA problem to them >:) . </p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2012/02/08/nutch-solr-term-vector-and-so-on/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Hadoop &#8211; java.io.IOException: error=12, Cannot allocate memory</title>
		<link>https://xiehang.com/blog/2012/02/07/hadoop-java-io-ioexception-error12-cannot-allocate-memory/</link>
					<comments>https://xiehang.com/blog/2012/02/07/hadoop-java-io-ioexception-error12-cannot-allocate-memory/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Tue, 07 Feb 2012 23:18:20 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[hadoop]]></category>
		<category><![CDATA[java]]></category>
		<category><![CDATA[memory]]></category>
		<category><![CDATA[swap]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1254</guid>

					<description><![CDATA[I hit this problem in my project which is hadoop-based: Cannot run program "chmod": java.io.IOException: error=12, Cannot allocate memory Did some research but found nothing useful &#8211; everybody mentioned it&#8217;s JDK&#8217;s problem not using fork()+exec() which caused excessive memory allocated during spawning new process for running shell command. However, it&#8217;s weird that I hit this <a href='https://xiehang.com/blog/2012/02/07/hadoop-java-io-ioexception-error12-cannot-allocate-memory/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>I hit this problem in my project which is hadoop-based:</p>
<pre>
Cannot run program "chmod": java.io.IOException: error=12, Cannot allocate memory
</pre>
<p>Did some research but found nothing useful &#8211; everybody mentioned it&#8217;s JDK&#8217;s problem not using fork()+exec() which caused excessive memory allocated during spawning new process for running shell command. However, it&#8217;s weird that I hit this problem on my AWS micro instance only, not on my MacBook, so I moved on to check some more &#8211;</p>
<p>It turned out swap is a problem, my micro instance in AWS does not have swap enabled (i.e. zero swap space), after add 1G swap everything&#8217;s fine now.</p>
<p>I&#8217;m a Java newbie, so my question is, though it got solved, did I do something properly?</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2012/02/07/hadoop-java-io-ioexception-error12-cannot-allocate-memory/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Now it&#8217;s HBase time</title>
		<link>https://xiehang.com/blog/2011/11/30/now-its-hbase-time/</link>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Wed, 30 Nov 2011 15:01:06 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[hadoop]]></category>
		<category><![CDATA[hbase]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1155</guid>

					<description><![CDATA[Just installed HBase on my Hadoop node, now I have a runnable HBase instance. There were some issues and I want to list things clearly: Hadoop version, remember to copy hadoop-core-*.jar (only one jar) from Hadoop deployment to hbase/lib get a copy of commons-configuration*.jar, the one in hadoop/lib directory should work]]></description>
										<content:encoded><![CDATA[<p>Just installed HBase on my Hadoop node, now I have a runnable HBase instance.</p>
<p>There were some issues and I want to list things clearly:</p>
<ul>
<li>Hadoop version, remember to copy hadoop-core-*.jar (only one jar) from Hadoop deployment to hbase/lib</li>
<li>get a copy of commons-configuration*.jar, the one in hadoop/lib directory should work</li>
</ul>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Playing with Hive</title>
		<link>https://xiehang.com/blog/2011/11/17/playing-with-hive/</link>
					<comments>https://xiehang.com/blog/2011/11/17/playing-with-hive/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Thu, 17 Nov 2011 15:46:09 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[apache]]></category>
		<category><![CDATA[hadoop]]></category>
		<category><![CDATA[log]]></category>
		<category><![CDATA[macbook]]></category>
		<category><![CDATA[map reduce]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1147</guid>

					<description><![CDATA[I got 700K lines of apache log files from a friend&#8217;s web server and imported them to the testing Hadoop instance running on my MacBoox, following the instructions listed here I successfully run some analysis. Note, the last section in the article talking about the Apache Weblog Data doesn&#8217;t seem to be correct &#8211; it <a href='https://xiehang.com/blog/2011/11/17/playing-with-hive/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>I got 700K lines of apache log files from a friend&#8217;s web server and imported them to the testing Hadoop instance running on my MacBoox, following the instructions listed <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">here</a> I successfully run some analysis.</p>
<p>Note, the last section in the article talking about the </i>Apache Weblog Data</i> doesn&#8217;t seem to be correct &#8211; it lacks of some space (&#8221; &#8220;) after ^ and it gave me quite some headache since I&#8217;m not familiar with Java regular expressions. Luckily <a href="https://issues.apache.org/jira/browse/HIVE-662">Hive issue 662</a> mentioned in the article gave me the correct regex to get things done.</p>
<p>It seems I can only learn to play with Hive/Hadoop cos Hadoop running on MacBook is still a single node installation which is &#8230; <b>SLOW</b>, but so far I&#8217;m fine with it as I don&#8217;t have high volume of data to be processed. As a reference, getting top accessed IPs (which I used to figure out potential abusers) took 83 seconds. The HSQL is simple, something like &#8220;select host, count(*) cc from apachelog group by host order by cc desc limit 10;&#8221;.</p>
<p>Hadoop is a richmen&#8217;s game, seems it only improve the performance whenever you have lots of nodes as it can well distributed tasks.</p>
<p>BTW, <i>Hadoop: The Definitive Guide</i> is a good book <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f642.png" alt="ðŸ™‚" class="wp-smiley" style="height: 1em; max-height: 1em;" /> .</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2011/11/17/playing-with-hive/feed/</wfw:commentRss>
			<slash:comments>6</slash:comments>
		
		
			</item>
		<item>
		<title>Started playing with Hadoop</title>
		<link>https://xiehang.com/blog/2011/11/02/started-playing-with-hadoop/</link>
					<comments>https://xiehang.com/blog/2011/11/02/started-playing-with-hadoop/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Wed, 02 Nov 2011 17:02:56 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[hadoop]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1139</guid>

					<description><![CDATA[Hadoop time, trying to setup a 6-nodes cluster and once it works will play with codes for some while.]]></description>
										<content:encoded><![CDATA[<p>Hadoop time, trying to setup a 6-nodes cluster and once it works will play with codes for some while.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2011/11/02/started-playing-with-hadoop/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			</item>
	</channel>
</rss>

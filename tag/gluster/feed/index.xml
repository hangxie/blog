<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>gluster &#8211; Flying Bug</title>
	<atom:link href="https://xiehang.com/blog/tag/gluster/feed/" rel="self" type="application/rss+xml" />
	<link>https://xiehang.com/blog</link>
	<description>Debugging and Being Debugged</description>
	<lastBuildDate>Thu, 18 Sep 2014 02:30:11 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.9.3</generator>
	<item>
		<title>Virtualization, GlusterFS, and automation</title>
		<link>https://xiehang.com/blog/2014/01/27/virtualization-glusterfs-and-automation/</link>
					<comments>https://xiehang.com/blog/2014/01/27/virtualization-glusterfs-and-automation/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Tue, 28 Jan 2014 00:55:10 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[automation]]></category>
		<category><![CDATA[centos]]></category>
		<category><![CDATA[gluster]]></category>
		<category><![CDATA[glusterfs]]></category>
		<category><![CDATA[kickstart]]></category>
		<category><![CDATA[libvirt]]></category>
		<category><![CDATA[virt-install]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1619</guid>

					<description><![CDATA[Finally I got some time playing with GlusterFS and libvirt. I&#8217;m not going to step into GlusterFS setup as it&#8217;s really easy and straight forward, I did CentOS kickstart (non-attendance) installation as well since it may help me in future deployment. Note that I assume you have glusterd running the host node, other all localhost <a href='https://xiehang.com/blog/2014/01/27/virtualization-glusterfs-and-automation/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Finally I got some time playing with GlusterFS and libvirt.</p>
<p>I&#8217;m not going to step into GlusterFS setup as it&#8217;s really easy and straight forward, I did CentOS kickstart (non-attendance) installation as well since it may help me in future deployment. Note that I assume you have glusterd running the host node, other all localhost below should be replaced by the hostname that is running glusterd.<span id="more-1619"></span></p>
<p>First, create the kickstart file somewhere accessible through HTTP, assuming it is http://192.168.0.1/kickstart.txt:<br />
[raw]<br />
install<br />
cdrom<br />
text<br />
lang en_US.UTF-8<br />
keyboard us<br />
network &#8211;onboot yes &#8211;device eth0 &#8211;bootproto dhcp &#8211;noipv6<br />
timezone &#8211;utc America/Los_Angeles<br />
rootpw  &#8211;iscrypted SOMETHING-COPIED-FROM-/ETC/SHADDOW<br />
selinux &#8211;disabled<br />
authconfig &#8211;enableshadow &#8211;passalgo=sha512<br />
firewall &#8211;service=ssh<br />
poweroff</p>
<p>zerombr yes<br />
clearpart &#8211;linux &#8211;drives=vda<br />
part /boot &#8211;fstype ext4 &#8211;size 512<br />
part swap &#8211;recommended<br />
part / &#8211;fstype ext4 &#8211;size 4096 &#8211;grow</p>
<p>bootloader &#8211;location=mbr &#8211;timeout=5 &#8211;append=&#8221;rhgb quiet console=ttyS0&#8243;</p>
<p>%packages &#8211;nobase<br />
sudo<br />
ypbind<br />
nfs-utils<br />
ntp<br />
bind-utils<br />
%end<br />
[/raw]</p>
<p>I&#8217;m not going to explain configuration options one by one as you can get more detailed explanation from <a href="http://www.centos.org/docs/5/html/Installation_Guide-en-US/s1-kickstart2-options.html">CentOS&#8217;s kickstart documentation site</a>.</p>
<p>Then, create the image file (raw format) on GlusterFS volume, you can do it on fuse mounted place at this moment since there won&#8217;t be any significant amount data to be written:<br />
[shell]<br />
sudo truncate -s 100G /mnt/glusterfs/VM/test-vm/disk.img<br />
[/shell]</p>
<p>Or you prefer qemu-img way:<br />
[shell]<br />
sudo qemu-img create -f raw gluster://localhost:24007/glusterfs_vol_name/VM/test-vm/disk.img 100G<br />
[/shell]</p>
<p>Either one will create a sparsed file size 100G.</p>
<p>Now, if you&#8217;ve downloaded the installation ISO to somewhere, do this:<br />
[shell]<br />
sudo virt-install &#8211;name test-vm &#8211;ram 1024 &#8211;vcpus 2 &#8211;disk path=/mnt/glusterfs/VM/test-vm/disk.img,size=100 &#8211;network bridge=br1 &#8211;os-variant rhel6 &#8211;accelerate &#8211;location /path/to/CentOS-6.2-x86_64-bin-DVD1.iso &#8211;graphics none -x &#8220;ks=http://192.168.0.1/kickstart.txt console=ttyS0&#8221;<br />
[/shell]</p>
<p>or if you prefer do a network installation:<br />
[shell]<br />
sudo virt-install &#8211;name test-vm &#8211;ram 1024 &#8211;vcpus 2 &#8211;disk path=/mnt/glusterfs/VM/test-vm/disk.img,size=100 &#8211;network bridge=br1 &#8211;os-variant rhel6 &#8211;accelerate &#8211;location http://mirror.stanford.edu/yum/pub/centos/6/os/x86_64/ &#8211;graphics none -x &#8220;ks=http://192.168.0.1/kickstart.txt console=ttyS0&#8221;<br />
[/shell]</p>
<p>After the installation, the machine should be powered-off, if for whatever reason it does not, do a poweroff, then do:<br />
<code><br />
sudo virsh list --all<br />
</code></p>
<p>to make sure test-vm is not running, then edit the VM&#8217;s configuration by:<br />
[shell]<br />
sudo virsh edit test-vm<br />
[/shell]</p>
<p>The only thing needs to be changed is the location of the disk image, it should be:<br />
[xml]<br />
    <disk type='file' device='disk'><br />
      <driver name='qemu' type='raw' cache='none'/><source file='/mnt/glusterfs/VM/test-vm/disk.img'/><target dev='vda' bus='virtio'/></p>
<address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </disk><br />
[/xml]</p>
<p>and need to be changed to:</p>
<p>[xml]<br />
    <disk type='network' device='disk'><br />
      <driver name='qemu' type='raw' cache='none'/><source protocol='gluster' name='glusterfs_vol_name/VM/test-vm/disk.img'><host name='localhost' port='24007'/><br />
      </source><br />
      <target dev='vda' bus='virtio'/></p>
<address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </disk><br />
[/xml]</p>
<p>Remember double check ownership of the disk.img file &#8211; it should be owned by qemu:qemu, otherwise you are going to run into disk IO problem (in VM).</p>
<p>Now, start the VM:<br />
[shell]<br />
sudo virsh start &#8211;console test-vm<br />
[/shell]</p>
<p>and everything should be there <img src="https://s.w.org/images/core/emoji/13.1.0/72x72/1f642.png" alt="ðŸ™‚" class="wp-smiley" style="height: 1em; max-height: 1em;" /> .</p>
<p>If you have problem with the VM and want to start over, you can:<br />
[shell]sudo virsh reset test-vm[/shell]<br />
to power-cycle the VM,<br />
[shell]sudo virsh destroy test-vm[/shell],<br />
to power-off the VM<br />
[shell]sudo virsh undefine test-vm[/shell],<br />
to remove the VM from libvirt&#8217;s repository.</p>
<p>I did some performance test on disk, putting the disk image on GlusterFS makes disk I/O slow for sure, but I think it is still acceptable:<br />
[shell]<br />
glusterfs-fuse   1048576000 bytes (1.0 GB) copied, 192.922 s, 5.4 MB/s<br />
glusterfs-api    1048576000 bytes (1.0 GB) copied, 33.0347 s, 31.7 MB/s<br />
local-filesystem 1048576000 bytes (1.0 GB) copied, 5.70798 s, 184 MB/s<br />
[/shell]</p>
<p>The test command was:<br />
[shell]<br />
dd if=/dev/zero of=/tmp/dd-test bs=1M count=1000<br />
[/shell]</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2014/01/27/virtualization-glusterfs-and-automation/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Planning GlusterFS</title>
		<link>https://xiehang.com/blog/2013/10/03/planning-glusterfs-2/</link>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Thu, 03 Oct 2013 20:25:33 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[gluster]]></category>
		<category><![CDATA[operation]]></category>
		<category><![CDATA[replace-brick]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1596</guid>

					<description><![CDATA[Learnt from the mailing list that I should not export mounting point as a brick directly, instead I should create a sub-directory under the mounting point and take it as brick, this will make life much easier later on whenever you have to do migration or re-organization (like changing replica). Developers of GlusterFS are thinking <a href='https://xiehang.com/blog/2013/10/03/planning-glusterfs-2/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Learnt from the mailing list that I should not export mounting point as a brick directly, instead I should create a sub-directory under the mounting point and take it as brick, this will make life much easier later on whenever you have to do migration or re-organization (like changing replica).</p>
<p>Developers of GlusterFS are thinking of retiring replace-brick, from the mailing list I can see they have good point to do so (especially existing codes for replace-brick is too bad to be improved :P), however, as a (sort of) ops guy, I still prefer they can keep it since it makes ops&#8217; life much more easier, but obviously they don&#8217;t quite agree with all requests/challenges to keep it. Anyway, it seems life will not end, it will just become a little bit troublesome, and if this is the trade-off for stability and/or performance, I&#8217;m fine with it.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Planning GlusterFS</title>
		<link>https://xiehang.com/blog/2013/08/02/planning-glusterfs/</link>
					<comments>https://xiehang.com/blog/2013/08/02/planning-glusterfs/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Fri, 02 Aug 2013 19:15:00 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[availability]]></category>
		<category><![CDATA[gluster]]></category>
		<category><![CDATA[reliability]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1563</guid>

					<description><![CDATA[Here&#8217;s my plan. I have 4 servers named them as gfs[11-14], each machine has 4 disks, with sda allocated for root, swap, etc, and sdb-sdd as data in XFS. I&#8217;m going to run distributed-replicated volume with replication 2, and I want to keep highest availability, as much as I can. So the layout will be <a href='https://xiehang.com/blog/2013/08/02/planning-glusterfs/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Here&#8217;s my plan.</p>
<p>I have 4 servers named them as gfs[11-14], each machine has 4 disks, with sda allocated for root, swap, etc, and sdb-sdd as data in XFS.</p>
<p>I&#8217;m going to run distributed-replicated volume with replication 2, and I want to keep highest availability, as much as I can. So the layout will be something like this:<span id="more-1563"></span></p>
<table border="1">
<tbody>
<tr>
<th></th>
<th>sdb</th>
<th>sdc</th>
<th>sdd</th>
</tr>
<tr>
<td>gfs11</td>
<td>brick1#R1</td>
<td>brick2#R1</td>
<td>brick3#R1</td>
</tr>
<tr>
<td>gfs12</td>
<td>brick1#R2</td>
<td>brick4#R1</td>
<td>brick5#R1</td>
</tr>
<tr>
<td>gfs13</td>
<td>brick2#R2</td>
<td>brick4#R2</td>
<td>brick6#R1</td>
</tr>
<tr>
<td>gfs14</td>
<td>brick3#R2</td>
<td>brick5#R2</td>
<td>brick6#R2</td>
</tr>
</tbody>
</table>
<p>So anytime:</p>
<ul>
<li>If there is single box failure, I have access to all data</li>
<li>If there are two boxes dead, I have access to 5/6 data</li>
<li>If there are three nodes dead, I still have access to half of data</li>
</ul>
<p>&nbsp;</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2013/08/02/planning-glusterfs/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
	</channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>glusterfs &#8211; Flying Bug</title>
	<atom:link href="https://xiehang.com/blog/tag/glusterfs/feed/" rel="self" type="application/rss+xml" />
	<link>https://xiehang.com/blog</link>
	<description>Debugging and Being Debugged</description>
	<lastBuildDate>Thu, 18 Sep 2014 02:30:11 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0.1</generator>
	<item>
		<title>Virtualization, GlusterFS, and automation</title>
		<link>https://xiehang.com/blog/2014/01/27/virtualization-glusterfs-and-automation/</link>
					<comments>https://xiehang.com/blog/2014/01/27/virtualization-glusterfs-and-automation/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Tue, 28 Jan 2014 00:55:10 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[automation]]></category>
		<category><![CDATA[centos]]></category>
		<category><![CDATA[gluster]]></category>
		<category><![CDATA[glusterfs]]></category>
		<category><![CDATA[kickstart]]></category>
		<category><![CDATA[libvirt]]></category>
		<category><![CDATA[virt-install]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1619</guid>

					<description><![CDATA[Finally I got some time playing with GlusterFS and libvirt. I&#8217;m not going to step into GlusterFS setup as it&#8217;s really easy and straight forward, I did CentOS kickstart (non-attendance) installation as well since it may help me in future deployment. Note that I assume you have glusterd running the host node, other all localhost <a href='https://xiehang.com/blog/2014/01/27/virtualization-glusterfs-and-automation/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Finally I got some time playing with GlusterFS and libvirt.</p>
<p>I&#8217;m not going to step into GlusterFS setup as it&#8217;s really easy and straight forward, I did CentOS kickstart (non-attendance) installation as well since it may help me in future deployment. Note that I assume you have glusterd running the host node, other all localhost below should be replaced by the hostname that is running glusterd.<span id="more-1619"></span></p>
<p>First, create the kickstart file somewhere accessible through HTTP, assuming it is http://192.168.0.1/kickstart.txt:<br />
[raw]<br />
install<br />
cdrom<br />
text<br />
lang en_US.UTF-8<br />
keyboard us<br />
network &#8211;onboot yes &#8211;device eth0 &#8211;bootproto dhcp &#8211;noipv6<br />
timezone &#8211;utc America/Los_Angeles<br />
rootpw  &#8211;iscrypted SOMETHING-COPIED-FROM-/ETC/SHADDOW<br />
selinux &#8211;disabled<br />
authconfig &#8211;enableshadow &#8211;passalgo=sha512<br />
firewall &#8211;service=ssh<br />
poweroff</p>
<p>zerombr yes<br />
clearpart &#8211;linux &#8211;drives=vda<br />
part /boot &#8211;fstype ext4 &#8211;size 512<br />
part swap &#8211;recommended<br />
part / &#8211;fstype ext4 &#8211;size 4096 &#8211;grow</p>
<p>bootloader &#8211;location=mbr &#8211;timeout=5 &#8211;append=&#8221;rhgb quiet console=ttyS0&#8243;</p>
<p>%packages &#8211;nobase<br />
sudo<br />
ypbind<br />
nfs-utils<br />
ntp<br />
bind-utils<br />
%end<br />
[/raw]</p>
<p>I&#8217;m not going to explain configuration options one by one as you can get more detailed explanation from <a href="http://www.centos.org/docs/5/html/Installation_Guide-en-US/s1-kickstart2-options.html">CentOS&#8217;s kickstart documentation site</a>.</p>
<p>Then, create the image file (raw format) on GlusterFS volume, you can do it on fuse mounted place at this moment since there won&#8217;t be any significant amount data to be written:<br />
[shell]<br />
sudo truncate -s 100G /mnt/glusterfs/VM/test-vm/disk.img<br />
[/shell]</p>
<p>Or you prefer qemu-img way:<br />
[shell]<br />
sudo qemu-img create -f raw gluster://localhost:24007/glusterfs_vol_name/VM/test-vm/disk.img 100G<br />
[/shell]</p>
<p>Either one will create a sparsed file size 100G.</p>
<p>Now, if you&#8217;ve downloaded the installation ISO to somewhere, do this:<br />
[shell]<br />
sudo virt-install &#8211;name test-vm &#8211;ram 1024 &#8211;vcpus 2 &#8211;disk path=/mnt/glusterfs/VM/test-vm/disk.img,size=100 &#8211;network bridge=br1 &#8211;os-variant rhel6 &#8211;accelerate &#8211;location /path/to/CentOS-6.2-x86_64-bin-DVD1.iso &#8211;graphics none -x &#8220;ks=http://192.168.0.1/kickstart.txt console=ttyS0&#8221;<br />
[/shell]</p>
<p>or if you prefer do a network installation:<br />
[shell]<br />
sudo virt-install &#8211;name test-vm &#8211;ram 1024 &#8211;vcpus 2 &#8211;disk path=/mnt/glusterfs/VM/test-vm/disk.img,size=100 &#8211;network bridge=br1 &#8211;os-variant rhel6 &#8211;accelerate &#8211;location http://mirror.stanford.edu/yum/pub/centos/6/os/x86_64/ &#8211;graphics none -x &#8220;ks=http://192.168.0.1/kickstart.txt console=ttyS0&#8221;<br />
[/shell]</p>
<p>After the installation, the machine should be powered-off, if for whatever reason it does not, do a poweroff, then do:<br />
<code><br />
sudo virsh list --all<br />
</code></p>
<p>to make sure test-vm is not running, then edit the VM&#8217;s configuration by:<br />
[shell]<br />
sudo virsh edit test-vm<br />
[/shell]</p>
<p>The only thing needs to be changed is the location of the disk image, it should be:<br />
[xml]<br />
    <disk type='file' device='disk'><br />
      <driver name='qemu' type='raw' cache='none'/><source file='/mnt/glusterfs/VM/test-vm/disk.img'/><target dev='vda' bus='virtio'/></p>
<address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </disk><br />
[/xml]</p>
<p>and need to be changed to:</p>
<p>[xml]<br />
    <disk type='network' device='disk'><br />
      <driver name='qemu' type='raw' cache='none'/><source protocol='gluster' name='glusterfs_vol_name/VM/test-vm/disk.img'><host name='localhost' port='24007'/><br />
      </source><br />
      <target dev='vda' bus='virtio'/></p>
<address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </disk><br />
[/xml]</p>
<p>Remember double check ownership of the disk.img file &#8211; it should be owned by qemu:qemu, otherwise you are going to run into disk IO problem (in VM).</p>
<p>Now, start the VM:<br />
[shell]<br />
sudo virsh start &#8211;console test-vm<br />
[/shell]</p>
<p>and everything should be there <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f642.png" alt="ðŸ™‚" class="wp-smiley" style="height: 1em; max-height: 1em;" /> .</p>
<p>If you have problem with the VM and want to start over, you can:<br />
[shell]sudo virsh reset test-vm[/shell]<br />
to power-cycle the VM,<br />
[shell]sudo virsh destroy test-vm[/shell],<br />
to power-off the VM<br />
[shell]sudo virsh undefine test-vm[/shell],<br />
to remove the VM from libvirt&#8217;s repository.</p>
<p>I did some performance test on disk, putting the disk image on GlusterFS makes disk I/O slow for sure, but I think it is still acceptable:<br />
[shell]<br />
glusterfs-fuse   1048576000 bytes (1.0 GB) copied, 192.922 s, 5.4 MB/s<br />
glusterfs-api    1048576000 bytes (1.0 GB) copied, 33.0347 s, 31.7 MB/s<br />
local-filesystem 1048576000 bytes (1.0 GB) copied, 5.70798 s, 184 MB/s<br />
[/shell]</p>
<p>The test command was:<br />
[shell]<br />
dd if=/dev/zero of=/tmp/dd-test bs=1M count=1000<br />
[/shell]</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2014/01/27/virtualization-glusterfs-and-automation/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Playing with Gluster File System</title>
		<link>https://xiehang.com/blog/2013/08/05/playing-with-gluster-file-system/</link>
					<comments>https://xiehang.com/blog/2013/08/05/playing-with-gluster-file-system/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Mon, 05 Aug 2013 22:58:45 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[glusterfs]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1568</guid>

					<description><![CDATA[Working on glusterfs test environment as mentioned here, so far everything&#8217;s working with some headache. have 4 nodes up and running and join into same pool, make all extra disks (sdb, sdc, sdd) XFS (fdisk then mkfs, so sdb mentioned below is actually sdb1) create distributed-replicated volume gfs_v0 with gfs11:sdb+gfs12:sdb and gfs11:sdc+gfs12:sdc mount gfs on <a href='https://xiehang.com/blog/2013/08/05/playing-with-gluster-file-system/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Working on glusterfs test environment as mentioned <a href="https://xiehang.com/blog/2013/08/02/planning-glusterfs/">here</a>, so far everything&#8217;s working with some headache.</p>
<ol>
<li>have 4 nodes up and running and join into same pool, make all extra disks (sdb, sdc, sdd) XFS (fdisk then mkfs, so sdb mentioned below is actually sdb1)</li>
<li>create distributed-replicated volume gfs_v0 with gfs11:sdb+gfs12:sdb and gfs11:sdc+gfs12:sdc</li>
<li>mount gfs on all these 4 boxes (I don&#8217;t have dedicated client hosts &#8230;)</li>
<li>copied 16 450M~500M tgz files to gfs_v0, everything looked fine</li>
<p><span id="more-1568"></span></p>
<li>untar two tgz files to gfs_v0, result in 256 directories, with ~1100 files in each directory, it was working smoothly, though the performance is not that great, but I think this is a known issue, or by design to behave like this</li>
<li>add gfs13:sdd+gfs14:sdd to gfs_v0, then launch rebalance, both worked as expected and rebalance is faster than I was expecting (compare with the untar in step #5, but I think glusterfs is actually writing the XFS disks so performance is better)</li>
<li>here comes the trouble case &#8211; since current layout does not meet my requirement as mentioned in <a href="https://xiehang.com/blog/2013/08/02/planning-glusterfs/">planning page</a>, I need to use gfs13:sdb replace gfs12:sdc, &#8220;replace-brick start&#8221; without problem, but &#8220;replace-brick status&#8221; told me &#8220;cannot commit on localhost&#8221; or something similar, it seems this is a known issue and lots of people got hit by this, but so far I haven&#8217;t found any fix yet</li>
<li>kick out a &#8220;replace-brick commit force&#8221; (maybe I did wrong in this step, let&#8217;s see), and vol info shows that the replacement was done, but obviously without data properly transferred</li>
<li>I think I should do a heal at that moment, but who knows, I launched rebalance, which seems ok and healing is ongoing as well, but after ~10 hours it is still on-going, so I stopped rebalance, and launched heal to make sure at least the volume is good enough to handle disaster. It was really fast so I guess most stuffs got fixed in the rebalance stage.</li>
<li>add three more pairs of bricks gfs11:sdd+gfs14:sdb, gfs12:sdc+gfs13:sdc, and gfs12:sdd+gfs14:sdc, and launched rebalance</li>
</ol>
<p>I&#8217;m waiting for the last rebalance to be finished and then I&#8217;m going to create some disasters :D. Things like power off a node, power cycle a node, fail a disk, all these sort of stuffs and let&#8217;s see what happen then.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2013/08/05/playing-with-gluster-file-system/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>To-do list with glusterfs</title>
		<link>https://xiehang.com/blog/2013/07/22/to-do-list-with-glusterfs/</link>
					<comments>https://xiehang.com/blog/2013/07/22/to-do-list-with-glusterfs/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Tue, 23 Jul 2013 00:41:31 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[glusterfs]]></category>
		<category><![CDATA[performance]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1555</guid>

					<description><![CDATA[Playing with GlusterFS now, here&#8217;s the to-do list: Installation and basic configuration, plus getting familiar with command line utilities Set up RAID-10-like configuration, with geo-replication if it is possible Regular routine maintenance, haven&#8217;t got clear idea yet, but should include: expand a volume, shrink a volume, replace a brick in volume, re-balance data, convert another <a href='https://xiehang.com/blog/2013/07/22/to-do-list-with-glusterfs/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Playing with <a href="http://www.gluster.org/">GlusterFS </a>now, here&#8217;s the to-do list:</p>
<ol>
<li><span style="line-height: 13px;">Installation and basic configuration, plus getting familiar with command line utilities</span></li>
<li>Set up RAID-10-like configuration, with geo-replication if it is possible</li>
<li>Regular routine maintenance, haven&#8217;t got clear idea yet, but should include: expand a volume, shrink a volume, replace a brick in volume, re-balance data, convert another fs to glusterfs, recover from various disasters, etc.</li>
<li>Performance testing with various scenarios, even people have certain number with them, include: mail server with maildir (large number of small files with small amount of concurrent access), file server (medium number of files and medium size with almost no concurrent access), video server (small number of large files with large amount of concurrent access), file based database like SQLite and BerkeleyDB (concurrent access with lots of seek operation), and RDBMS like mysql/postgre.</li>
</ol>
<p>Sound like a great plan, right :D? Let&#8217;s see.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2013/07/22/to-do-list-with-glusterfs/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
	</channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>cluster &#8211; Flying Bug</title>
	<atom:link href="https://xiehang.com/blog/tag/cluster/feed/" rel="self" type="application/rss+xml" />
	<link>https://xiehang.com/blog</link>
	<description>Debugging and Being Debugged</description>
	<lastBuildDate>Tue, 28 Jan 2014 18:03:54 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.9.3</generator>
	<item>
		<title>Slowly making progress on XMPP</title>
		<link>https://xiehang.com/blog/2013/10/31/slowly-making-progress-on-xmpp/</link>
					<comments>https://xiehang.com/blog/2013/10/31/slowly-making-progress-on-xmpp/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Fri, 01 Nov 2013 00:36:09 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[cluster]]></category>
		<category><![CDATA[ejabberd]]></category>
		<category><![CDATA[pubsub]]></category>
		<category><![CDATA[sleekxmpp]]></category>
		<category><![CDATA[websocket]]></category>
		<category><![CDATA[xmpp]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1605</guid>

					<description><![CDATA[Alright, a quick summary on XMPP before Halloween ðŸ˜‰ ejabberd is up and running with 3 nodes in the cluster, and having data replicated to all 3 nodes thus service will be up and running as long as there is at least one node alive. With SleekXMPP created 5M which is much faster then ejabberdctl, <a href='https://xiehang.com/blog/2013/10/31/slowly-making-progress-on-xmpp/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Alright, a quick summary on XMPP before Halloween <img src="https://s.w.org/images/core/emoji/13.1.0/72x72/1f609.png" alt="ðŸ˜‰" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>ejabberd is up and running with 3 nodes in the cluster, and having data replicated to all 3 nodes thus service will be up and running as long as there is at least one node alive. With SleekXMPP created 5M which is much faster then ejabberdctl, created 10,000 pubsub nodes so that I can do announcement to user groups, but seems I need to recreate them as they are currently all belong to admin account, which does not fit into real world. pubsub is working as well, but I need to find another solution with better performance &#8211; it&#8217;s current about 1 subscribe/second, and seems most time was spent on authentication, so I need a feature of &#8220;subscribe for others&#8221; to speed up the test deployment, though this is not necessary in real production. I haven&#8217;t test publishing information yet.<span id="more-1605"></span></p>
<p>Next step I need to do information publishing, for online and offline users, plus test how many messages I can queue, and who can do the announcement. Also, or more important, I need to test mnesia fragment which will make the cluster truly expandable. I think I&#8217;m going to create some &#8220;core&#8221; nodes to store fragmented data, and some other non-core nodes to handle connections. Once this is tested I believe everything is good for production.</p>
<p>I won&#8217;t do anything regarding websockets as target client will be an App, also standard for XMPP over websocket is still quite unstable thus it could waste more if I work on it more.</p>
<p>Pretty much everything&#8217;s here &#8230;</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2013/10/31/slowly-making-progress-on-xmpp/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>ejabberd cluster</title>
		<link>https://xiehang.com/blog/2013/10/14/ejabberd-cluster/</link>
					<comments>https://xiehang.com/blog/2013/10/14/ejabberd-cluster/#comments</comments>
		
		<dc:creator><![CDATA[Hang]]></dc:creator>
		<pubDate>Tue, 15 Oct 2013 06:31:52 +0000</pubDate>
				<category><![CDATA[Triviality]]></category>
		<category><![CDATA[cluster]]></category>
		<category><![CDATA[ejabberd]]></category>
		<category><![CDATA[ha]]></category>
		<category><![CDATA[xmpp]]></category>
		<guid isPermaLink="false">https://xiehang.com/blog/?p=1601</guid>

					<description><![CDATA[Time to play with HA ejabberd setup now. It was said number of daily active user will be less than 1M, and IM is just a feature of the mobile product, and believe that people will spend less then 2 hours on it everyday, also people will use the App mainly during traffic hours. So <a href='https://xiehang.com/blog/2013/10/14/ejabberd-cluster/' class='excerpt-more'>[...]</a>]]></description>
										<content:encoded><![CDATA[<p>Time to play with HA ejabberd setup now.</p>
<p>It was said number of daily active user will be less than 1M, and IM is just a feature of the mobile product, and believe that people will spend less then 2 hours on it everyday, also people will use the App mainly during traffic hours. So it&#8217;s 2M hours span to 7~9am and 6~8pm, 4 hours. I would like to assume it is evenly distributed, this makes the concurrent online user &#8230; 500K, Not a big number, and since it is just a non-key feature, so activities will be limited.</p>
<p>So I think I&#8217;d go with a full replicated ejabberd cluster, &#8220;full&#8221; means I&#8217;m going to replicate everything &#8211; password, roster, offline message. I will check the stats of the production environment to see how the resource utilization is going, and tune the allocation whenever necessary.<span id="more-1601"></span></p>
<p>Regarding the setup, pretty straightforward once I went through once (I did test on Ubuntu 12.04 LTS, but production eventually will be CentOS6):</p>
<p>1. setup node#1, change host (actually, it is domain for the XMPP service) in /etc/ejabberd/ejabberd.cfg, add mod_http_bind to modules (so to enable http-bind and http-poll), and change EJABBERD_NODE from &#8216;ejabberd&#8217; to &#8216;ejabberd@`hostname -f`&#8217; in /etc/default/ejabberd. You need to remove everything (except .erlang.cookie) under /var/lib/ejabberd then restart so to have the right mnesia DB.<br />
2. setup node#2, same as node#1, before restart ejabberd, copy .erlang.cookie from node#1<br />
3. run &#8220;ejabberdctl debug&#8221; as ejabberd, then:<br />
<code><br />
FirstNode = 'ejabberd@node#1', mnesia:stop(), mnesia:delete_schema([node()]), mnesia:start(), mnesia:change_config(extra_db_nodes, [FirstNode]), mnesia:change_table_copy_type(schema, node(), disc_copies).<br />
</code><br />
note that node#1 should be FQDN<br />
4. CTRL-C twice to quit (q(). used to work, but I don&#8217;t know what changed to erlang which makes it no longer works), then restart ejabberd. Run &#8220;ejabberdctl status&#8221; and &#8220;ejabberd mnesia info&#8221; to make sure the service is properly running on this host, and it is has node#1 included in DB node<br />
5. repeat 2~4 on all other nodes (I have 4 nodes in total)<br />
6. after all set, you can go to http://node/#1:5280/admin/ to make sure all nodes are in the cluster</p>
<p>Now &#8230; I believe it&#8217;s doable in ejabberdctl debug, but still could not figure out how. However, it&#8217;s easier to do it through web admin interface. I just change everything exactly the same as node#1 on all other nodes (RAM, disc, disc and RAM, etc). After that, do a reboot on all machines then use ejabberdctl status (or even with &#8211;nodes Node#N) to make sure everything can be up and running after a reboot.</p>
<p>I&#8217;m going to do LVS for XMPP port and nginx load balancer for 5280 (http-bind), will post result later.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://xiehang.com/blog/2013/10/14/ejabberd-cluster/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
	</channel>
</rss>
